{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN) Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tensorflow\n",
    "%pip install tensorflow_datasets==4.9.2\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what data to use for model\n",
    "data_type = 'REAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and clean Real or Synthetic data\n",
    "\n",
    "if data_type == 'REAL':\n",
    "    data = pd.read_csv(\"../../00_data/final_data.csv\")\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"top_critics\"] = df[\"top_critics\"].str.split(r\"\\., |\\. ,\")\n",
    "    df = df.explode(\"top_critics\").reset_index(drop=True)\n",
    "    df['winner'] = df['winner'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    reviews = df['top_critics'].astype(str)\n",
    "    tags = df['winner'].astype(int)\n",
    "\n",
    "elif data_type == 'SYNTHETIC':\n",
    "    data = pd.read_csv(\"../../00_data/generated_reviews.csv\")\n",
    "    df = pd.DataFrame(data)\n",
    "    df['winner'] = df['winner'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    reviews = df['generated_reviews']\n",
    "    tags = df['winner'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to tensorflow compatible datasets\n",
    "combined_data = list(zip(reviews, tags))\n",
    "def generator():\n",
    "    for review, tag in combined_data:\n",
    "        yield review, tag\n",
    "output_types = (tf.string, tf.int32)\n",
    "output_shapes = (tf.TensorShape([]), tf.TensorShape([]))\n",
    "new_dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=output_types, \n",
    "    output_shapes=output_shapes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test one sample review\n",
    "for example, label in new_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the TensorSpecs\n",
    "new_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the total number of elements in the dataset\n",
    "DATASET_SIZE = sum(1 for _ in new_dataset)\n",
    "\n",
    "# Calculate split sizes for 90/10 train/test split\n",
    "TRAIN_SIZE = int(0.9 * DATASET_SIZE)\n",
    "TEST_SIZE = DATASET_SIZE - TRAIN_SIZE\n",
    "\n",
    "# Shuffle the dataset\n",
    "SHUFFLE_BUFFER_SIZE = DATASET_SIZE  # Adjust this as needed for your dataset size and memory constraints\n",
    "shuffled_dataset = new_dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "# Create train dataset\n",
    "new_train_dataset = shuffled_dataset.take(TRAIN_SIZE)\n",
    "\n",
    "# Create test dataset\n",
    "new_test_dataset = shuffled_dataset.skip(TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of elements in the train dataset\n",
    "train_dataset_size = sum(1 for _ in new_train_dataset)\n",
    "\n",
    "# Count the number of elements in the test dataset\n",
    "test_dataset_size = sum(1 for _ in new_test_dataset)\n",
    "\n",
    "# Print number of elements in each dataset\n",
    "print(f\"Number of elements in the train dataset: {train_dataset_size}\")\n",
    "print(f\"Number of elements in the test dataset: {test_dataset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish buffer and batch sizes \n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-batch the training and test datasets\n",
    "train_dataset = new_train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = new_test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish Model Parameters and Run RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocab size and encoder function\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "vocab = np.array(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN model layers\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with Binary Cross Entropy Loss Function\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare plot\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and store the loss and accuracy for plotting\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=25,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Loss and Accuracy\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Accuracy\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Accuracy vs Epochs - Real Data\")\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Loss vs Epochs - Real Data\")\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix, Precision, Recall, and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Extract test data from Tensor Object\n",
    "predictions = []\n",
    "labels = []\n",
    "for new_inputs, new_labels in test_dataset:\n",
    "    results = model(new_inputs)\n",
    "    predictions.append(results)\n",
    "    labels.append(new_labels)\n",
    "\n",
    "# Initialize an empty list to store the floats\n",
    "predicted_floats = []\n",
    "\n",
    "# Iterate through each prediction tensor\n",
    "for tensor in predictions:\n",
    "    \n",
    "    # Convert the tensor to a numpy array and flatten it\n",
    "    numpy_array = tensor.numpy().flatten()\n",
    "    \n",
    "    # Extend the all_floats list with the elements from the numpy array\n",
    "    predicted_floats.extend(numpy_array.tolist())\n",
    "\n",
    "# Convert each tensor to a numpy array and concatenate them\n",
    "flattened_array = np.concatenate([tensor.numpy() for tensor in labels])\n",
    "\n",
    "# Convert the numpy array of labels to a list\n",
    "labels_list = flattened_array.tolist()\n",
    "\n",
    "# Apply sigmoid function to predicted probabilities\n",
    "probabilities = 1 / (1 + np.exp(-np.array(predicted_floats)))\n",
    "\n",
    "# Apply threshold to logit transformed probabilities\n",
    "classification = [1 if prob >= 0.5 else 0 for prob in probabilities]\n",
    "\n",
    "print(\"Neural Network Model - Confustion Matrix\")\n",
    "print(classification_report(labels_list, classification))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
