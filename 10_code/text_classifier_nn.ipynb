{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  \"\"\"\n",
    "    This code was modified from the original code by Omar Espejel https://github.com/omarespejel\n",
    "    \"\"\"\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import DBpedia\n",
    "\n",
    "# Torch text version\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = data[~data[\"top_critics\"].isnull()]\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, stratify=data[\"winner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_list = list(zip(train[\"winner\"], train[\"top_critics\"]))\n",
    "\n",
    "\n",
    "def tuple_generator(tuple_list):\n",
    "    for tup in tuple_list:\n",
    "        yield tup\n",
    "\n",
    "\n",
    "train_iter = tuple_generator(tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 'Sleepless has the puzzling, 95-minutes feel of a doomed project that has been mercilessly edited to digestible proportions., Its thin but taut and at a lean 95 minutes, it zips along at a propulsive, entertaining clip., The final twist is genuinely unexpected, though purely because of its irrelevance., A smart and speedy cop movie that occasionally reaches for greatness., Less sleepless, more insomnia cure. For all its attempts at action, its a rote, dull crime thriller with little fresh to offer., Alleged action movies need way more novelty than just flour being dusted over the combatants or brief bursts of underwater photography to ring the changes., The sheer silliness defeated me., In its portrayal of cops on the take, Sleepless often resembles an episode of TVs The Shield., Monaghan and Foxx, for all their gifts, cant transcend the material, though they do get more out of it than most others would be able to., The 95 minutes it takes for this sleepless night to unfold onscreen feel tedious rather than breathless., Sadly, even the foggy recollection of the original is better than this bloated bastardization., It might not be Donald Westlake, but it does its thing: meaningless, nonstop violence and movement, enacted by a large cast of characters who are only looking out to survive into the next scene., Alas, Andrea Berloffs screenplay has more holes than a trawlermans net, none bigger than a gut-cut Foxx nursing a gaping wound for two-thirds of the picture while still managing to dodge bullets and roll with the punches., We get muttering and glowering from Mr. Foxx, a story that cant manage enough twists to tie a shoelace, and set pieces that have been done better in other movies., Mostly, everyone goes dutifully through the motions, including Foxx, whose level of investment in the project is neutral at best., The listless, shoddy sort of remake where it feels like all of the characters have already seen the movie theyre in, Sleepless reduces one of the best action films of the 21st century into one of the most benign., One of those movies that needed to be a lot better or a lot worse to make much of an impression., Never rising above the level of generic B-movie, Sleepless represents the sort of disposable fare typically dropped into theaters in January., Sleepless is a propulsive thin exercise, energetic but tedious, the kind of January movie that Jamie Foxx should have permanently graduated from., At a certain point, Odars intense atmospherics - amplified by the throbbing bass notes of Michael Kamms heavy, percussive score - start to feel like the work of a filmmaker on genre autopilot.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "# This besides tokenize the data convert words into integers according to the vocab available\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list = []\n",
    "    text_list = []\n",
    "    offsets = [0]\n",
    "\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(texto_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class TextClassifierModel(nn.Module):\n",
    "#     def __init__(self, vocab_size, embed_dim, num_class):\n",
    "#         super(TextClassifierModel, self).__init__()\n",
    "\n",
    "#         # (embedding)\n",
    "#         self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "\n",
    "#         # (batch normalization)\n",
    "#         self.bn1 = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "#         # (fully connected)\n",
    "#         self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "#     def forward(self, text, offsets):\n",
    "#         # Incrustar el texto (embed the text)\n",
    "#         embedded = self.embedding(text, offsets)\n",
    "\n",
    "#         # Aplicar la normalización por lotes (apply batch normalization)\n",
    "#         embedded_norm = self.bn1(embedded)\n",
    "\n",
    "#         # Aplicar la función de activación ReLU (apply the ReLU activation function)\n",
    "#         embedded_activated = F.relu(embedded_norm)\n",
    "\n",
    "#         # Devolver las probabilidades de clase (output the class probabilities)\n",
    "#         return self.fc(embedded_activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TextClassifierModelWithRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_class):\n",
    "        super(TextClassifierModelWithRNN, self).__init__()\n",
    "\n",
    "        # (embedding)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # (LSTM layer)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "\n",
    "        # (batch normalization)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # (fully connected)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Incrustar el texto (embed the text)\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        # Apply the LSTM layer\n",
    "        output, _ = self.rnn(embedded)\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        # Apply batch normalization\n",
    "        output_norm = self.bn1(output)\n",
    "\n",
    "        # Apply the ReLU activation function\n",
    "        output_activated = F.relu(output_norm)\n",
    "\n",
    "        # Return the class probabilities\n",
    "        return self.fc(output_activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_list = list(zip(train[\"winner\"], train[\"top_critics\"]))\n",
    "\n",
    "\n",
    "def tuple_generator(tuple_list):\n",
    "    for tup in tuple_list:\n",
    "        yield tup\n",
    "\n",
    "\n",
    "train_iter = tuple_generator(tuple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_class = len(set([label for (label, text) in train_iter]))\n",
    "# vocab_size = len(vocab)\n",
    "# embedding_size = 100\n",
    "\n",
    "# model = TextClassifierModel(\n",
    "#     vocab_size=vocab_size, embed_dim=embedding_size, num_class=num_class\n",
    "# ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 100\n",
    "\n",
    "model = TextClassifierModelWithRNN(\n",
    "    vocab_size=vocab_size, embed_dim=embedding_size, hidden_size=64, num_class=num_class\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,573,019 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(dataloader):\n",
    "    # Putting the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize accuracy, count and loss for each epoch\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        # reset the gradients after each batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get predictions from the model\n",
    "        predict = model(text, offsets)\n",
    "\n",
    "        # Get the loss\n",
    "        loss = get_loss(predict, label)\n",
    "\n",
    "        # backpropagate the loss and compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Getting the accuracy\n",
    "        acc = (predict.argmax(1) == label).sum()\n",
    "\n",
    "        # Avoiding large gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Upgrading the epoch accuracy, count and loss\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_loss += loss.item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "        if idx % 50 == 0 and idx > 0:\n",
    "            print(\n",
    "                f\" epoch {epoch} | {idx}/{len(dataloader)} batches | loss {epoch_loss/total_count} | accuracy {epoch_acc/total_count}\"\n",
    "            )\n",
    "\n",
    "    return epoch_acc / total_count, epoch_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(dataloader):\n",
    "    model.eval()\n",
    "    epoch_acc = 0\n",
    "    total_count = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            # Obtenemos la la etiqueta predecida\n",
    "            #\n",
    "            prediction = model(text, offsets)\n",
    "\n",
    "            # Obtenemos pérdida y accuracy\n",
    "            loss = get_loss(prediction, label)\n",
    "            acc = (prediction.argmax(1) == label).sum()\n",
    "\n",
    "            # Llevamos el conteo de la pérdida y el accuracy para esta epoch\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "    return epoch_acc / total_count, epoch_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "\n",
    "EPOCHS = 4  # epochs\n",
    "LEARNING_RATE = 0.2  # tasa de aprendizaje\n",
    "# BATCH_SIZE = 64 # tamaño de los batches\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "\n",
    "tuple_list = list(zip(train[\"winner\"], train[\"top_critics\"]))\n",
    "tuple_list_test = list(zip(test[\"winner\"], test[\"top_critics\"]))\n",
    "\n",
    "\n",
    "def tuple_generator(tuple_list):\n",
    "    for tup in tuple_list:\n",
    "        yield tup\n",
    "\n",
    "\n",
    "train_iter = tuple_generator(tuple_list)\n",
    "test_iter = tuple_generator(tuple_list_test)\n",
    "\n",
    "\n",
    "# Getting training and testing datasets\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# 95% from the trainset and the remaining 5% for validation\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "# Generating data loaders to be used in the model\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/rnn_rd278/rnn.ipynb Cell 22\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# training\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     training_acc, training_loss \u001b[39m=\u001b[39m training_model(train_dataloader)\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Validation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     validation_acc, valitacion_loss \u001b[39m=\u001b[39m eval_model(valid_dataloader)\n",
      "\u001b[1;32m/workspaces/rnn_rd278/rnn.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Get predictions from the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m predict \u001b[39m=\u001b[39m model(text, offsets)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Get the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m get_loss(predict, label)\n",
      "File \u001b[0;32m/workspaces/rnn_rd278/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspaces/rnn_rd278/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/workspaces/rnn_rd278/rnn.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embedded)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Take the output from the last time step\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m output \u001b[39m=\u001b[39m output[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, :]\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Apply batch normalization\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bverbose-disco-5vg5jv956xwf7q9q/workspaces/rnn_rd278/rnn.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m output_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(output)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Greating the greatest loss\n",
    "major_loss_validation = float(\"inf\")\n",
    "\n",
    "# training\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # training\n",
    "    training_acc, training_loss = training_model(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    validation_acc, valitacion_loss = eval_model(valid_dataloader)\n",
    "\n",
    "    # Guarda el mejor modelo\n",
    "    if valitacion_loss < major_loss_validation:\n",
    "        best_valid_loss = valitacion_loss\n",
    "        torch.save(model.state_dict(), \"best_saved.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset accuracy-> 0.7402597402597403\n",
      "test dataset loss-> 0.0277736589506075\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss = eval_model(test_dataloader)\n",
    "\n",
    "print(f\"Test dataset accuracy-> {test_acc}\")\n",
    "print(f\"test dataset loss-> {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscars_label = {\n",
    "    1: \"Non considered for the award\",\n",
    "    2: \"Nominated for best picture\",\n",
    "    3: \"Winner\",\n",
    "}\n",
    "\n",
    "\n",
    "def predict(text, texto_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(texto_pipeline(text))\n",
    "        opt_mod = torch.compile(model, mode=\"reduce-overhead\")\n",
    "        output = opt_mod(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>winner</th>\n",
       "      <th>top_critics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The King's Speech</td>\n",
       "      <td>2</td>\n",
       "      <td>Firth strikes a perfect balance between his ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Black Swan</td>\n",
       "      <td>1</td>\n",
       "      <td>What were they trying to say? What happened?.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inception</td>\n",
       "      <td>1</td>\n",
       "      <td>Inception engaged on a mainly intellectually l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 movie  winner  \\\n",
       "0   The King's Speech        2   \n",
       "1          Black Swan        1   \n",
       "3           Inception        1   \n",
       "\n",
       "                                         top_critics  \n",
       "0  Firth strikes a perfect balance between his ab...  \n",
       "1  What were they trying to say? What happened?.....  \n",
       "3  Inception engaged on a mainly intellectually l...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs for unknown reason\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will be Non considered for the award\n"
     ]
    }
   ],
   "source": [
    "sample_1 = \"That the director turned this most devastating of stories into \\\n",
    "a riveting pop culture phenomenon without ceding one inch on its tragic \\\n",
    "dimensions is surely an achievement for the ages. This is a complex look \\\n",
    "at a complicated man, but Oppenheimer unequivocally establishes that \\\n",
    "this is a story worth telling -- and that Nolan was the perfect filmmaker to do it.\\\n",
    "Every scene feels like a cataclysm waiting to happen, fitting for a film that builds,\\\n",
    "step-by-step, to the creation of a cataclysm machine. Oppenheimer both summons awe for\\\n",
    "what it took to build the bomb and for the changes it wrought.\"\n",
    "\n",
    "sample_2 = data[\"top_critics\"][0]\n",
    "\n",
    "# print(f\"Oppenheimer will be {oscars_label[predict(sample_1, texto_pipeline)]}\")\n",
    "\n",
    "print(f\" will be {oscars_label[predict(sample_2, texto_pipeline)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "winner\n",
       "0    0.740157\n",
       "1    0.236220\n",
       "2    0.023622\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"winner\"].value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs for unknown reason\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0, 225,   0],\n",
       "       [  0,  72,   0],\n",
       "       [  0,   7,   0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing confussion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = train[\"winner\"]\n",
    "y_pred = [predict(text, texto_pipeline) for text in train[\"top_critics\"]]\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
